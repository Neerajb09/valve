{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AortaDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, sov_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.sov_folder = sov_folder\n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_folder, self.image_files[idx].replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
    "        sov_path = os.path.join(self.sov_folder, self.image_files[idx].replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error loading image: {image_path}\")\n",
    "        if len(image.shape) == 2:  # Convert grayscale images to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        image = cv2.resize(image, (640, 640))  # Resize for YOLO input\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0  # Normalize\n",
    "\n",
    "        # Load bounding box labels\n",
    "        label = torch.zeros(4)  # Default label (x, y, w, h)\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                parts = f.readline().strip().split()\n",
    "                if len(parts) == 5:  # Ensure it's a valid bounding box\n",
    "                    label = torch.tensor(list(map(float, parts[1:])), dtype=torch.float32)\n",
    "\n",
    "        # Load SOV coordinates\n",
    "        sov_coords = torch.zeros(4)  # Default (x, y, w, h)\n",
    "        if os.path.exists(sov_path):\n",
    "            with open(sov_path, \"r\") as f:\n",
    "                parts = f.readline().strip().split()\n",
    "                if len(parts) == 5:  # Ensure valid file\n",
    "                    sov_coords = torch.tensor(list(map(float, parts[1:])), dtype=torch.float32)\n",
    "\n",
    "        return image, sov_coords, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class YOLOWithMLP(nn.Module):\n",
    "    def __init__(self, mlp_hidden_dim, mlp_output_dim):\n",
    "        super(YOLOWithMLP, self).__init__()\n",
    "\n",
    "        # Load YOLOv8n (pretrained) for feature extraction\n",
    "        self.yolo = YOLO(\"yolov8n.pt\").to(\"cuda:6\")  \n",
    "        self.yolo.eval()  # Set YOLO to evaluation mode (no gradient updates)\n",
    "\n",
    "        # Freeze YOLO parameters (so only the MLP is trained)\n",
    "        for param in self.yolo.model.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        # Extract feature size dynamically (No dummy input needed at runtime)\n",
    "        sample_input = torch.randn(1, 3, 640, 640).to(\"cuda:6\")\n",
    "        with torch.no_grad():\n",
    "            features = self.yolo.model.model[:10](sample_input)\n",
    "            feature_dim = features.shape[1]  # Extract number of channels\n",
    "\n",
    "        print(f\"Detected YOLO feature size from 10th layer: {feature_dim}\")\n",
    "\n",
    "        # Define MLP (Trainable part)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feature_dim + 4, mlp_hidden_dim),  # YOLO features + additional 4D input\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, mlp_output_dim),  # Output (e.g., x, y, w, h)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, images, additional_features):\n",
    "        with torch.no_grad():  # YOLO runs inference only (no gradients)\n",
    "            features = self.yolo.model.model[:10](images)  # Extract 10th layer features\n",
    "        \n",
    "        # Apply Global Average Pooling (GAP) to create a fixed-size vector\n",
    "        yolo_features = features.mean(dim=[2, 3])  \n",
    "\n",
    "        # Concatenate YOLO features with additional numerical input\n",
    "        combined_features = torch.cat((yolo_features, additional_features), dim=1)\n",
    "\n",
    "        # Pass through the MLP (Trainable)\n",
    "        output = self.mlp(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected YOLO feature size from 10th layer: 256\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randn(1, 3, 640, 640).to(\"cuda:6\")  # Dummy image input\n",
    "sample_sov_coords = torch.randn(1, 4).to(\"cuda:6\")  # Dummy 4-coordinates\n",
    "\n",
    "yolo_model = YOLOWithMLP(256, 4).to(\"cuda:6\")\n",
    "sample_output = yolo_model(sample_input, sample_sov_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected YOLO feature size from 10th layer: 256\n",
      "Epoch [1/100], Loss: 0.0106\n",
      "Epoch [2/100], Loss: 0.0068\n",
      "Epoch [3/100], Loss: 0.0037\n",
      "Epoch [4/100], Loss: 0.0025\n",
      "Epoch [5/100], Loss: 0.0020\n",
      "Epoch [6/100], Loss: 0.0060\n",
      "Epoch [7/100], Loss: 0.0077\n",
      "Epoch [8/100], Loss: 0.0106\n",
      "Epoch [9/100], Loss: 0.0032\n",
      "Epoch [10/100], Loss: 0.0052\n",
      "Epoch [11/100], Loss: 0.0032\n",
      "Epoch [12/100], Loss: 0.0051\n",
      "Epoch [13/100], Loss: 0.0025\n",
      "Epoch [14/100], Loss: 0.0032\n",
      "Epoch [15/100], Loss: 0.0027\n",
      "Epoch [16/100], Loss: 0.0050\n",
      "Epoch [17/100], Loss: 0.0026\n",
      "Epoch [18/100], Loss: 0.0028\n",
      "Epoch [19/100], Loss: 0.0025\n",
      "Epoch [20/100], Loss: 0.0017\n",
      "Epoch [21/100], Loss: 0.0037\n",
      "Epoch [22/100], Loss: 0.0022\n",
      "Epoch [23/100], Loss: 0.0029\n",
      "Epoch [24/100], Loss: 0.0032\n",
      "Epoch [25/100], Loss: 0.0025\n",
      "Epoch [26/100], Loss: 0.0058\n",
      "Epoch [27/100], Loss: 0.0018\n",
      "Epoch [28/100], Loss: 0.0020\n",
      "Epoch [29/100], Loss: 0.0029\n",
      "Epoch [30/100], Loss: 0.0024\n",
      "Epoch [31/100], Loss: 0.0006\n",
      "Epoch [32/100], Loss: 0.0010\n",
      "Epoch [33/100], Loss: 0.0015\n",
      "Epoch [34/100], Loss: 0.0017\n",
      "Epoch [35/100], Loss: 0.0034\n",
      "Epoch [36/100], Loss: 0.0024\n",
      "Epoch [37/100], Loss: 0.0018\n",
      "Epoch [38/100], Loss: 0.0011\n",
      "Epoch [39/100], Loss: 0.0025\n",
      "Epoch [40/100], Loss: 0.0017\n",
      "Epoch [41/100], Loss: 0.0015\n",
      "Epoch [42/100], Loss: 0.0019\n",
      "Epoch [43/100], Loss: 0.0016\n",
      "Epoch [44/100], Loss: 0.0019\n",
      "Epoch [45/100], Loss: 0.0008\n",
      "Epoch [46/100], Loss: 0.0014\n",
      "Epoch [47/100], Loss: 0.0009\n",
      "Epoch [48/100], Loss: 0.0012\n",
      "Epoch [49/100], Loss: 0.0005\n",
      "Epoch [50/100], Loss: 0.0045\n",
      "Epoch [51/100], Loss: 0.0018\n",
      "Epoch [52/100], Loss: 0.0006\n",
      "Epoch [53/100], Loss: 0.0010\n",
      "Epoch [54/100], Loss: 0.0006\n",
      "Epoch [55/100], Loss: 0.0014\n",
      "Epoch [56/100], Loss: 0.0017\n",
      "Epoch [57/100], Loss: 0.0026\n",
      "Epoch [58/100], Loss: 0.0008\n",
      "Epoch [59/100], Loss: 0.0006\n",
      "Epoch [60/100], Loss: 0.0006\n",
      "Epoch [61/100], Loss: 0.0010\n",
      "Epoch [62/100], Loss: 0.0009\n",
      "Epoch [63/100], Loss: 0.0009\n",
      "Epoch [64/100], Loss: 0.0007\n",
      "Epoch [65/100], Loss: 0.0005\n",
      "Epoch [66/100], Loss: 0.0016\n",
      "Epoch [67/100], Loss: 0.0012\n",
      "Epoch [68/100], Loss: 0.0010\n",
      "Epoch [69/100], Loss: 0.0006\n",
      "Epoch [70/100], Loss: 0.0006\n",
      "Epoch [71/100], Loss: 0.0010\n",
      "Epoch [72/100], Loss: 0.0005\n",
      "Epoch [73/100], Loss: 0.0008\n",
      "Epoch [74/100], Loss: 0.0027\n",
      "Epoch [75/100], Loss: 0.0004\n",
      "Epoch [76/100], Loss: 0.0008\n",
      "Epoch [77/100], Loss: 0.0006\n",
      "Epoch [78/100], Loss: 0.0012\n",
      "Epoch [79/100], Loss: 0.0007\n",
      "Epoch [80/100], Loss: 0.0005\n",
      "Epoch [81/100], Loss: 0.0011\n",
      "Epoch [82/100], Loss: 0.0011\n",
      "Epoch [83/100], Loss: 0.0006\n",
      "Epoch [84/100], Loss: 0.0012\n",
      "Epoch [85/100], Loss: 0.0004\n",
      "Epoch [86/100], Loss: 0.0005\n",
      "Epoch [87/100], Loss: 0.0014\n",
      "Epoch [88/100], Loss: 0.0005\n",
      "Epoch [89/100], Loss: 0.0008\n",
      "Epoch [90/100], Loss: 0.0015\n",
      "Epoch [91/100], Loss: 0.0009\n",
      "Epoch [92/100], Loss: 0.0008\n",
      "Epoch [93/100], Loss: 0.0004\n",
      "Epoch [94/100], Loss: 0.0007\n",
      "Epoch [95/100], Loss: 0.0006\n",
      "Epoch [96/100], Loss: 0.0004\n",
      "Epoch [97/100], Loss: 0.0008\n",
      "Epoch [98/100], Loss: 0.0003\n",
      "Epoch [99/100], Loss: 0.0012\n",
      "Epoch [100/100], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Create dataset and dataloader\n",
    "train_dataset = AortaDataset(\"/mnt/nvme_disk2/User_data/nb57077k/cardiovision/phase2/Dataset/aorta_data/images/train\", \"/mnt/nvme_disk2/User_data/nb57077k/cardiovision/phase2/Dataset/aorta_data/labels/train\", \"/mnt/nvme_disk2/User_data/nb57077k/cardiovision/phase2/Dataset/aorta_data/SOV_labels/train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# Define model\n",
    "mlp_hidden_dim = 128\n",
    "mlp_output_dim = 4  # Example: bounding box coordinates (x, y, w, h)\n",
    "model = YOLOWithMLP(mlp_hidden_dim, mlp_output_dim).to(\"cuda:6\")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.mlp.parameters(), lr=1e-4)  # Only MLP is trainable\n",
    "criterion = nn.MSELoss()  # Example loss function (modify as needed)\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for images, additional_features, targets in train_loader:  # Replace with real dataloader\n",
    "        images, additional_features, targets = images.to(\"cuda:6\"), additional_features.to(\"cuda:6\"), targets.to(\"cuda:6\")\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        outputs = model(images, additional_features)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update MLP weights\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully as trained_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model after training\n",
    "torch.save(model.state_dict(), \"trained_mlp.pth\")\n",
    "print(\"✅ Model saved successfully as trained_mlp.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected YOLO feature size from 10th layer: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:01<00:00, 46.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved in predicted_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define paths\n",
    "VAL_IMAGES_FOLDER = \"/mnt/nvme_disk2/User_data/nb57077k/cardiovision/phase2/Dataset/aorta_data/images/val\"  # Folder containing validation images\n",
    "SOV_LABELS_FOLDER = \"/mnt/nvme_disk2/User_data/nb57077k/cardiovision/phase2/Dataset/aorta_data/SOV_labels/val\"  # Folder containing corresponding SOV bounding boxes\n",
    "OUTPUT_TXT_FOLDER = \"predicted_labels\"  # Where predictions will be saved\n",
    "os.makedirs(OUTPUT_TXT_FOLDER, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Load trained model\n",
    "mlp_hidden_dim = 128\n",
    "mlp_output_dim = 4  # Predicting (x, y, w, h)\n",
    "model = YOLOWithMLP(mlp_hidden_dim, mlp_output_dim).to(\"cuda:6\")\n",
    "model.load_state_dict(torch.load(\"trained_mlp.pth\"))  # Load trained weights\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Get list of images\n",
    "image_files = [f for f in os.listdir(VAL_IMAGES_FOLDER) if f.endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "# Run inference on all images\n",
    "with torch.no_grad():\n",
    "    for img_name in tqdm(image_files):\n",
    "        # Load image\n",
    "        img_path = os.path.join(VAL_IMAGES_FOLDER, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = cv2.resize(img, (640, 640))  # Resize to match YOLO input size\n",
    "        img = torch.tensor(img).float().permute(2, 0, 1).unsqueeze(0).to(\"cuda:6\") / 255.0  # Normalize\n",
    "\n",
    "        # Load corresponding SOV bounding box\n",
    "        txt_filename = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        sov_txt_path = os.path.join(SOV_LABELS_FOLDER, txt_filename)\n",
    "\n",
    "        if os.path.exists(sov_txt_path):\n",
    "            with open(sov_txt_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) > 0:\n",
    "                    sov_bbox = list(map(float, lines[0].strip().split()[1:]))  # Skip class label (0)\n",
    "                else:\n",
    "                    sov_bbox = [0, 0, 0, 0]  # Default if file empty\n",
    "        else:\n",
    "            sov_bbox = [0, 0, 0, 0]  # Default if no SOV bbox found\n",
    "\n",
    "        # Convert SOV bbox to tensor\n",
    "        additional_features = torch.tensor([sov_bbox]).to(\"cuda:6\")\n",
    "\n",
    "        # Run inference\n",
    "        outputs = model(img, additional_features)\n",
    "        pred_bbox = outputs[0].cpu().tolist()  # Get predictions (x, y, w, h)\n",
    "\n",
    "        # Convert bbox format (normalize x, y, w, h between 0 and 1)\n",
    "        img_width, img_height = 640, 640  # Since all images are resized to 640x640\n",
    "        x, y, w, h = pred_bbox       \n",
    "\n",
    "        # Save to corresponding .txt file\n",
    "        pred_txt_path = os.path.join(OUTPUT_TXT_FOLDER, txt_filename)\n",
    "\n",
    "        with open(pred_txt_path, \"w\") as f:\n",
    "            f.write(f\"0 {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")  # Write in YOLO format\n",
    "\n",
    "print(f\"✅ Predictions saved in {OUTPUT_TXT_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harsh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
